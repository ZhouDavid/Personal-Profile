<!DOCTYPE html>
<!-- saved from url=(0063)http://benjaminbolte.com/blog/2016/keras-language-modeling.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Deep Language Modeling for Question Answering using Keras</title>
    <meta name="description" content="An in-depth introduction to using Keras for language modeling. Includes sections on word embedding, characterizing recurrent and convolutional neural networks, attentional RNNs, and similarity metrics for vector embeddings, each with example code.
">

    <link rel="stylesheet" href="./Deep Language Modeling for Question Answering using Keras_files/main.css">
    <link rel="canonical" href="http://codekansas.github.io/blog/2016/keras-language-modeling.html">
<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.0') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.0') format('woff'), url('https://cdn.mathjax.org/mathjax/2.7-latest/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.0') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>


  <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    <header class="site-header">

  <div class="wrapper">

    <nav class="site-nav">
      <a href="http://benjaminbolte.com/blog/2016/keras-language-modeling.html#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
        </svg>
      </a>

      <div class="trigger">
        <a class="page-link" href="http://benjaminbolte.com/">Home</a>
        
          
        
          
        
          
        
          
          <a class="page-link" href="http://benjaminbolte.com/ml/">Machine Learning</a>
          
        
          
        
          
          <a class="page-link" href="http://benjaminbolte.com/personal/">Personal</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>
 
 <!-- LaTeX formatting: Use $$equation$$ -->
 <script src="./Deep Language Modeling for Question Answering using Keras_files/MathJax.js.下载" type="text/javascript"></script>
 
 </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">
	<a name="top"></a>

  <header class="post-header">
    <h1 class="post-title">Deep Language Modeling for Question Answering using Keras</h1>
    <p class="post-meta">April 27, 2016</p>
  </header>

  <article class="post-content">
    <table class="note">
<tbody><tr><th>Github Repository</th></tr>
<tr><td>
The repository associated with this post can be found <a href="https://github.com/codekansas/keras-language-modeling" target="_blank">here</a>.
</td></tr>
</tbody></table>

<h1 id="introduction">Introduction</h1>

<p><a href="https://en.wikipedia.org/wiki/Question_answering">Question answering</a> has received more focus as large search engines have basically mastered general information retrieval and are starting to cover more edge cases. Question answering happens to be one of those edge cases, because it could involve a lot of syntatic nuance that doesn’t get captured by standard information retrieval models, like LDA or LSI. Hypothetically, deep learning models would be better suited to this type of task because of their ability to capture higher-order syntax. Two papers, “Applying deep learning to answer selection: a study and an open task” <a href="http://arxiv.org/pdf/1508.01585v2.pdf">(Feng et. al. 2015)</a> and “LSTM-based deep learning models for non-factoid answer selection” <a href="http://arxiv.org/pdf/1511.04108.pdf">(Tan et. al. 2016)</a>, are recent examples which have applied deep learning to question-answering tasks with good results.</p>

<p><a href="http://arxiv.org/pdf/1508.01585v2.pdf">Feng et. al.</a> used an in-house Java framework for their work, and <a href="http://arxiv.org/pdf/1511.04108.pdf">Tan et. al.</a> built their model entirely from Theano. Personally, I am a lot lazier than them, and I don’t understand CNNs very well, so I would like to use an existing framework to build one of their models to see if I could get similar results. <a href="https://github.com/fchollet/keras">Keras</a> is a really popular one that has support for everything we might need to put the model together.</p>

<h1 id="installing-keras">Installing Keras</h1>

<p>See the instructions <a href="http://keras.io/#installation">here</a> on how to install Keras. The simple route is to install using <code class="highlighter-rouge">pip</code>, e.g.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sudo pip install --upgrade keras</code></pre></figure>

<p>There are some important features that might not be available without the most recent version. I’m not sure if doing <code class="highlighter-rouge">pip install</code> gets the most recent version, so it might be helpful to install from binary. This is actually pretty straightforward! Just change to the directory where you want your source code to be and do:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">git clone https://github.com/fchollet/keras.git
<span class="nb">cd </span>keras
sudo python setup.py install</code></pre></figure>

<p>One benefit of this is that if you want to add a custom layer, you can add it to the Keras installation and be able to use it across different projects. Even better, you could fork the project and clone your own fork, although this gets into areas of Git beyond my understanding.</p>

<h1 id="preliminaries">Preliminaries</h1>

<p>There are actually a couple language models in the <a href="https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py">Keras examples</a>:</p>

<ul>
  <li><a href="https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py"><code class="highlighter-rouge">imdb_lstm.py</code></a>: Using a LSTM recurrent neural network to do sentiment analysis on the IMDB dataset</li>
  <li><a href="https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py"><code class="highlighter-rouge">imdb_cnn_lstm.py</code></a> The same task, but this time using a CNN layer beneath the LSTM layer</li>
  <li><a href="https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py"><code class="highlighter-rouge">babi_rnn.py</code></a>: Recurrent neural networks for modeling Facebook’s bAbi dataset, “a mixture of 20 tasks for testing text understanding and reasoning”</li>
</ul>

<p>These are pretty interesting to play around with. It is really cool how easy it is to get one of these set up! With Keras, a high-level model design can be quickly implemented.</p>

<h1 id="word-embeddings">Word Embeddings</h1>

<p>Ok! Let’s dive in. The first challenge that you might think of when designing a language model is what the units of the language might be. A reasonable dataset might have around 20000 distinct words, after lemmatizing them. If the average sentence is 40 words long, then you’re left with a <code class="highlighter-rouge">20000 x 40</code> matrix just to represent one sentence, which is 3.2 megabytes if each word is represented in 32 bits. This obviously doesn’t work, so the first step in developing a good language model is to figure out how to reduce the number of dimensions required to represent a word.</p>

<p>One popular method of doing this is using <code class="highlighter-rouge">word2vec</code>. <code class="highlighter-rouge">word2vec</code> is a way of embedding words in a vector space so that words that are semantically similar are near each other. There are some interesting consequences of doing this, like being able to do word addition and subtraction:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">king - man + woman <span class="o">=</span> queen</code></pre></figure>

<p>In Keras, this is available as an <code class="highlighter-rouge">Embedding</code> layer. This layer takes as input a <code class="highlighter-rouge">(n_batches, sentence_length)</code> dimensional matrix of integers representing each word in the corpus, and outputs a <code class="highlighter-rouge">(n_batches, sentence_length, n_embedding_dims)</code> dimensional matrix, where the last dimension is the word embedding.</p>

<p>There are two advantages to this. The first is space: Instead of 3.2 megabytes, a 40 word sentence embedded in 100 dimensions would only take 16 kilobytes, which is much more reasonable. More importantly, word embeddings give the model a hint at the meaning of each word, so it will converge more quickly. There are significantly fewer parameters which have to be jostled around, and parameters are sort of tied together in a sensible way so that they jostle in the right direction.</p>

<p>Here’s how you would go about writing something like this:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Embedding</span>

<span class="n">input_sentence</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">sentence_maxlen</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int32'</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_words</span><span class="p">,</span> <span class="n">n_embed_dims</span><span class="p">)(</span><span class="n">input_sentence</span><span class="p">)</span></code></pre></figure>

<p>Let’s try this out! We can train a recurrent neural network to predict some dummy data and examine the embedding layer for each vector. This model takes a sentence like “sam is red” or “sarah not green” and predicts what color the person is. It is a very simple example, but it will illustrate what the Embedding layer is doing, and also illustrate how we can turn a bunch of sentences into vectors of indices by building a dictionary.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="s">'''
sam is red
hannah not red
hannah is green
bob is green
bob not red
sam not green
sarah is red
sarah not green'''</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="n">is_green</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int32'</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="n">lemma</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span>
<span class="n">sentences_lemmatized</span> <span class="o">=</span> <span class="p">[</span><span class="n">lemma</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">sentences_lemmatized</span><span class="p">))</span>
<span class="c"># set(['boy', 'fed', 'ate', 'cat', 'kicked', 'hat'])</span>

<span class="c"># dictionaries for converting words to integers and vice versa</span>
<span class="n">word2idx</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">v</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">idx2word</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c"># convert the sentences a numpy array</span>
<span class="n">to_idx</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">sentences_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_idx</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences_lemmatized</span><span class="p">]</span>
<span class="n">sentences_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sentences_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int32'</span><span class="p">)</span>

<span class="c"># parameters for the model</span>
<span class="n">sentence_maxlen</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">n_embed_dims</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c"># put together a model to predict </span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">merge</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">SimpleRNN</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">input_sentence</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">sentence_maxlen</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int32'</span><span class="p">)</span>
<span class="n">input_embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_words</span><span class="p">,</span> <span class="n">n_embed_dims</span><span class="p">)(</span><span class="n">input_sentence</span><span class="p">)</span>
<span class="n">color_prediction</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">input_embedding</span><span class="p">)</span>

<span class="n">predict_green</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">input_sentence</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="n">color_prediction</span><span class="p">])</span>
<span class="n">predict_green</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">)</span>

<span class="c"># fit the model to predict what color each person is</span>
<span class="n">predict_green</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">sentences_array</span><span class="p">],</span> <span class="p">[</span><span class="n">is_green</span><span class="p">],</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">predict_green</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span>

<span class="c"># print out the embedding vector associated with each word</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
	<span class="k">print</span><span class="p">(</span><span class="s">'{}: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx2word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></code></pre></figure>

<p>The embedding layer embeds the words into 3 dimensions. A sample of the vectors it produces is seen below. As predicted, the model learns useful word embeddings.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sarah:	<span class="o">[</span>-0.5835458  -0.2772688   0.01127077]
sam:	<span class="o">[</span>-0.57449967 -0.26132962  0.04002968]

bob:	<span class="o">[</span> 1.10480607  0.97720605  0.10953052]
hannah:	<span class="o">[</span> 1.12466967  0.95199704  0.13520472]

not:	<span class="o">[</span>-0.17611612 -0.2958962  -0.06028322]
is:	<span class="o">[</span>-0.10752882 -0.34842652 -0.06909169]

red:	<span class="o">[</span>-0.10381682 -0.31055665 -0.0975003 <span class="o">]</span>
green:	<span class="o">[</span>-0.05930901 -0.33241618 -0.06948926]</code></pre></figure>

<p>Each category is grouped in the 3-dimensional vector space. The network learned each of these categories from how each word was used; Sarah and Sam are the red people, while Bob and Hannah are the green people. However, it did not differentiate well between <code class="highlighter-rouge">not</code>, <code class="highlighter-rouge">is</code>, <code class="highlighter-rouge">red</code>, and <code class="highlighter-rouge">green</code>, because those weren’t immediately obvious for the decision task.</p>

<p><img src="./Deep Language Modeling for Question Answering using Keras_files/word_vectors.png" alt="Word distributions in vector space"></p>

<h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>

<p>As the Keras examples illustrate, there are different philosophies on deep language modeling. <a href="http://arxiv.org/pdf/1508.01585v2.pdf">Feng et. al.</a> did a bunch of benchmarks with convolutional networks, and ended up with some impressive results. <a href="http://arxiv.org/pdf/1511.04108.pdf">Tan et. al.</a> used recurrent networks with some different parameters. I’ll focus on recurrent neural networks first (What do pirates call neural networks? <em>Arrrgh</em>NNs). I’ll assume some familiarity with both recurrent and convolutional neural networks. <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy’s blog</a> discusses recurrent neural networks in detail. Here is an image from that post which explains the core concept:</p>

<p><img src="./Deep Language Modeling for Question Answering using Keras_files/karpathy_rnn.jpeg" alt="Recurrent neural network"></p>

<h2 id="vanilla">Vanilla</h2>

<p>The basic RNN architecture is essentially a feed-forward neural network that is stretched out over a bunch of time steps and has it’s intermediate output added to the next input step. This idea can be expressed as an update equation for each input step:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">new_hidden_state <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U<span class="o">)</span> + b<span class="o">)</span></code></pre></figure>

<p>Note that <code class="highlighter-rouge">dot</code> indicates vector-matrix multiplication. Multiplying a vector of dimensions <code class="highlighter-rouge">&lt;m&gt;</code> by a matrix of dimensions <code class="highlighter-rouge">&lt;m, n&gt;</code> can be done with <code class="highlighter-rouge">dot(&lt;m&gt;, &lt;m, n&gt;)</code> and yields a vector of dimensions <code class="highlighter-rouge">&lt;n&gt;</code>. This is consistent with its usage in Theano and Keras. In the update equation, we multiply each <code class="highlighter-rouge">input_vector</code> by our input weights <code class="highlighter-rouge">W</code>, multiply the <code class="highlighter-rouge">prev_hidden</code> vector by our hidden weights <code class="highlighter-rouge">U</code>, and add a bias, before passing the sum to the activation function <code class="highlighter-rouge">sigmoid</code>. To get the <strong>many to one</strong> behavior in the image, we can grab the last hidden state and use that as our output. To get the <strong>one to many</strong> behavior, we can pass one input vector and then just pass a bunch of zero vectors to get as many hidden states as we want.</p>

<h2 id="lstm">LSTM</h2>

<p>If the RNN gets really long, then we run into a lot of difficulty training the model. The effect of something a early in the sequence on the end result is very small relative to later components, so it is hard to use that information in updating the weights. To solve this, several methods have been proposed, and two have been implemented in Keras. The first is the Long Short-Term Memory (LSTM) unit, which was proposed by <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">Hochreiter and Schmidhuber 1997</a>. This model uses a second hidden state which stores information from further back in the model, allowing that information to have a stronger effect on the end result. The update equations for this model are:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">input_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_input<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_input<span class="o">)</span> + b_input<span class="o">)</span>
forget_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_forget<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_forget<span class="o">)</span> + b_forget<span class="o">)</span>
output_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_output<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_output<span class="o">)</span> + b_output<span class="o">)</span>

candidate_state <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>x, W_hidden<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_hidden<span class="o">)</span> + b_hidden<span class="o">)</span>
memory_unit <span class="o">=</span> prev_candidate_state <span class="k">*</span> forget_gate + candidate_state <span class="k">*</span> input_gate

new_hidden_state <span class="o">=</span> tanh<span class="o">(</span>memory_unit<span class="o">)</span> <span class="k">*</span> output_gate</code></pre></figure>

<p>Note that <code class="highlighter-rouge">*</code> indicates element-wise multiplication. This is consistent with its usage in Theano and Keras. First, there are a bunch more parameters to train; not only do we have weights for the input-to-hidden and hidden-to-hidden matrices, but also we have an accompanying <code class="highlighter-rouge">candidate_state</code>. The candidate state is like a second hidden state that transfers information to and from the hidden state. It is like a safety deposit box for putting things in and taking things out.</p>

<h2 id="gru">GRU</h2>

<p>The second model is the Gated Recurrent Unit (GRU), which was proposed by <a href="http://arxiv.org/pdf/1406.1078.pdf">Cho et. al. 2014</a>. The equations for this model are as follows:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">update_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_update<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_update<span class="o">)</span> + b_update<span class="o">)</span>
reset_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_reset<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_reset<span class="o">)</span> + b_reset<span class="o">)</span>

reset_hidden <span class="o">=</span> prev_hidden <span class="k">*</span> reset_gate
temp_state <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_hidden<span class="o">)</span> + dot<span class="o">(</span>reset_hidden, U_reset<span class="o">)</span> + b_hidden<span class="o">)</span>
new_hidden_state <span class="o">=</span> <span class="o">(</span>1 - update_gate<span class="o">)</span> <span class="k">*</span> temp_state + update_gate <span class="k">*</span> prev_hidden</code></pre></figure>

<p>In this model, there is an <code class="highlighter-rouge">update_gate</code> which controls how much of the previous hidden state to carry over to the new hidden state and a <code class="highlighter-rouge">reset_gate</code> which controls how much the previous hidden state changes. This allows potentially long-term dependencies to be propagated through the network.</p>

<p>My implementations of these models in Theano, as well as optimizers for training them, can be found in <a href="https://github.com/codekansas/theano-rnn">this Github repository</a>.</p>

<h2 id="rnn-example-predicting-dummy-data">RNN Example: Predicting Dummy Data</h2>

<p>Now that we’ve seen the equations, let’s see how Keras implementations compare on some sample data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c"># parameters</span>
<span class="n">input_dims</span><span class="p">,</span> <span class="n">output_dims</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n_test</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c"># generate some random data to train on</span>
<span class="n">get_rand</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">shape</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">get_rand</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test</span><span class="p">)])</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">get_rand</span><span class="p">(</span><span class="n">output_dims</span><span class="p">,)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test</span><span class="p">)])</span>

<span class="c"># put together rnn models</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras.layers.recurrent</span> <span class="kn">import</span> <span class="n">SimpleRNN</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">GRU</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">theano</span>

<span class="n">input_sequence</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>

<span class="n">vanilla</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">output_dims</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">)(</span><span class="n">input_sequence</span><span class="p">)</span>
<span class="n">lstm</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">output_dims</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">)(</span><span class="n">input_sequence</span><span class="p">)</span>
<span class="n">gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">output_dims</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">)(</span><span class="n">input_sequence</span><span class="p">)</span>
<span class="n">rnns</span> <span class="o">=</span> <span class="p">[</span><span class="n">vanilla</span><span class="p">,</span> <span class="n">lstm</span><span class="p">,</span> <span class="n">gru</span><span class="p">]</span>

<span class="c"># train the models</span>
<span class="k">for</span> <span class="n">rnn</span> <span class="ow">in</span> <span class="n">rnns</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">input_sequence</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="n">rnn</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X_data</span><span class="p">],</span> <span class="p">[</span><span class="n">y_data</span><span class="p">],</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span></code></pre></figure>

<p>The results will vary from trial to trial. RNNs are exceptionally difficult to train. However, in general, a model that can take advantage of long-term dependencies will have a much easier time seeing how two sequences are different.</p>

<h1 id="attentional-rnns">Attentional RNNs</h1>

<p>It isn’t strictly important to understand the RNN part before looking at this part, but it will help everything make more sense. The next component of language modeling, which was the focus of the <a href="http://arxiv.org/pdf/1511.04108.pdf">Tan</a> paper, is the Attentional RNN. This essential components of model are described in “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention” <a href="http://arxiv.org/pdf/1502.03044.pdf">(Xu et. al. 2016)</a>. I’ll try to hash it out in this blog post a little bit and look at how to build it in Keras.</p>

<h2 id="lambda-layer">Lambda Layer</h2>

<p>First, let’s look at how to make a custom layer in Keras. There are a couple options. One is the <code class="highlighter-rouge">Lambda</code> layer, which does a specified operation. An example of this could be a layer that doubles the value it is passed:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int32'</span><span class="p">)</span>
<span class="n">double</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="nb">input</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="n">double</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">))</span></code></pre></figure>

<p>This doubles our input data. Note that there are no trainable weights anywhere in this model, so it couldn’t actually learn anything. What if we wanted to multiply our input vector by some trainable scalar that predicts the output vector? In this case, we will have to write our own layer.</p>

<h2 id="building-a-custom-layer-example">Building a Custom Layer Example</h2>

<p>Let’s jump right in and write a layer that learns to multiply an input by a scalar value and produce an output.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.engine</span> <span class="kn">import</span> <span class="n">Layer</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">initializations</span>

<span class="c"># our layer will take input shape (nb_samples, 1)</span>
<span class="k">class</span> <span class="nc">MultiplicationLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'glorot_uniform'</span><span class="p">)</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">MultiplicationLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
	
	<span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
		<span class="c"># each sample should be a scalar</span>
		<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">multiplicand</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">name</span><span class="o">=</span><span class="s">'multiplicand'</span><span class="p">)</span>
		
		<span class="c"># let Keras know that we want to train the multiplicand</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">multiplicand</span><span class="p">]</span>
	
	<span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
		<span class="c"># we're doing a scalar multiply, so we don't change the input shape</span>
		<span class="k">assert</span> <span class="n">input_shape</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
		<span class="k">return</span> <span class="n">input_shape</span>

	<span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
		<span class="c"># this is called during MultiplicationLayer()(input)</span>
		<span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiplicand</span>

<span class="c"># test the model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c"># input is a single scalar</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int32'</span><span class="p">)</span>
<span class="n">multiply</span> <span class="o">=</span> <span class="n">MultiplicationLayer</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="nb">input</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="n">multiply</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">output_data</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">input_data</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">input_data</span><span class="p">],</span> <span class="p">[</span><span class="n">output_data</span><span class="p">],</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">multiplicand</span><span class="o">.</span><span class="n">get_value</span><span class="p">())</span>
<span class="c"># should be close to 3</span></code></pre></figure>

<p>There we go! We have a complete model. We could change it around to make it fancier, like adding a <em>broadcastable dimension</em> to the <code class="highlighter-rouge">multiplicand</code> so that the layer could be passed a vector of numbers instead of just a scalar. Let’s look closer at how we built the multiplication layer:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'glorot_uniform'</span><span class="p">)</span>
	<span class="nb">super</span><span class="p">(</span><span class="n">MultiplicationLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code></pre></figure>

<p>First, we make a weight initializer that we can use later to get weights. <code class="highlighter-rouge">glorot_uniform</code> is just a particular way to initialize weights. We then call the <code class="highlighter-rouge">__init__</code> method of the super class.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
	<span class="c"># each sample should be a scalar</span>
	<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">multiplicand</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">name</span><span class="o">=</span><span class="s">'multiplicand'</span><span class="p">)</span>
	
	<span class="c"># let Keras know that we want to train the multiplicand</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">multiplicand</span><span class="p">]</span></code></pre></figure>

<p>This method specifies the components of the model, for when we build it. The only component we need is the scalar to multiply by, so we initialize a new tensor by calling <code class="highlighter-rouge">self.init</code>, the initializer we created in the <code class="highlighter-rouge">__init__</code> method.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
	<span class="c"># we're doing a scalar multiply, so we don't change the input shape</span>
	<span class="k">assert</span> <span class="n">input_shape</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
	<span class="k">return</span> <span class="n">input_shape</span></code></pre></figure>

<p>This method tells the builder what the output shape of this layer will be given its input shape. Since our layer just does a scalar multiply, it doesn’t change the output shape from the input shape. For example, scalar multiplying the input <code class="highlighter-rouge">[1, 2, 3]</code> of dimensions <code class="highlighter-rouge">&lt;3, 1&gt;</code> by a scalar factor of 2 gives the output <code class="highlighter-rouge">[2, 4, 6]</code>, which has the same dimensions <code class="highlighter-rouge">&lt;3, 1&gt;</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
	<span class="c"># this is called during MultiplicationLayer()(input)</span>
	<span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiplicand</span></code></pre></figure>

<p>This is the bread and butter of the the layer, where we actually perform the operation. We specify that the output of this layer is the input <code class="highlighter-rouge">x</code> matrix multiplied by our multiplicand tensor. Note that this method takes a while to run, because whatever backend we use (for example, Theano) has to put together the tensors in the right way. To make your layer run quickly, it is good practice to add <code class="highlighter-rouge">assert</code> checks in the <code class="highlighter-rouge">build</code> and <code class="highlighter-rouge">get_output_shape_for</code> methods.</p>

<h2 id="characterizing-the-attentional-lstm">Characterizing the Attentional LSTM</h2>

<p>Now that we’ve got an idea of how to build a custom layer, let’s look at the specifications for an attentional LSTM. Following <a href="http://arxiv.org/pdf/1511.04108.pdf">Tan et. al.</a>, we can augment our LSTM equations from earlier to include an attentional component. The attentional component requires some attention vector <code class="highlighter-rouge">attention_vec</code>.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">input_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_input<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_input<span class="o">)</span> + b_input<span class="o">)</span>
forget_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_forget<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_forget<span class="o">)</span> + b_forget<span class="o">)</span>
output_gate <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_output<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_output<span class="o">)</span> + b_output<span class="o">)</span>

candidate_state <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>input_vector, W_hidden<span class="o">)</span> + dot<span class="o">(</span>prev_hidden, U_hidden<span class="o">)</span> + b_hidden<span class="o">)</span>
memory_unit <span class="o">=</span> prev_candidate_state <span class="k">*</span> forget_gate + candidate_state <span class="k">*</span> input_gate

new_hidden_state <span class="o">=</span> tanh<span class="o">(</span>memory_unit<span class="o">)</span> <span class="k">*</span> output_gate

attention_state <span class="o">=</span> tanh<span class="o">(</span>dot<span class="o">(</span>attention_vec, W_attn<span class="o">)</span> + dot<span class="o">(</span>new_hidden_state, U_attn<span class="o">))</span>
attention_param <span class="o">=</span> exp<span class="o">(</span>dot<span class="o">(</span>attention_state, W_param<span class="o">))</span>
new_hidden_state <span class="o">=</span> new_hidden_state <span class="k">*</span> attention_param</code></pre></figure>

<p>The new equations are the last three, which correspond to equations 9, 10 and 11 from the paper (approximately reproduced below, using different notation).</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display"><span class="MathJax MathJax_FullWidth" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;tanh&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;U&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mo mathvariant=&quot;bold&quot; stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 100%; display: inline-block; min-width: 13.932em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(4.17em 1013.16em 8.693em -999.997em); top: -5.116em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-2"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.039em 1013.16em 4.408em -999.997em); top: -3.985em; left: 50%; margin-left: -6.604em;"><span class="msubsup" id="MathJax-Span-3"><span style="display: inline-block; position: relative; width: 0.896em; height: 0px;"><span style="position: absolute; clip: rect(3.336em 1000.42em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Main-bold;">s</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.807em; left: 0.479em;"><span class="texatom" id="MathJax-Span-7"><span class="mrow" id="MathJax-Span-8"><span class="mi" id="MathJax-Span-9" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-10" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-12" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-13" style="font-family: MathJax_Main; padding-left: 0.301em;">=</span><span class="mi" id="MathJax-Span-14" style="font-family: MathJax_Main; padding-left: 0.301em;">tanh</span><span class="mo" id="MathJax-Span-15"></span><span class="mo" id="MathJax-Span-16" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-17"><span class="mrow" id="MathJax-Span-18"><span class="mi" id="MathJax-Span-19" style="font-family: MathJax_Main-bold;">h</span></span></span><span class="mo" id="MathJax-Span-20" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-21" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-22" style="font-family: MathJax_Main;">)</span><span class="msubsup" id="MathJax-Span-23"><span style="display: inline-block; position: relative; width: 1.67em; height: 0px;"><span style="position: absolute; clip: rect(3.098em 1001.19em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-24"><span class="mrow" id="MathJax-Span-25"><span class="mi" id="MathJax-Span-26" style="font-family: MathJax_Main-bold;">W</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.807em; left: 1.193em;"><span class="texatom" id="MathJax-Span-27"><span class="mrow" id="MathJax-Span-28"><span class="mi" id="MathJax-Span-29" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-30" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="msubsup" id="MathJax-Span-31" style="padding-left: 0.241em;"><span style="display: inline-block; position: relative; width: 1.074em; height: 0px;"><span style="position: absolute; clip: rect(3.396em 1000.6em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-32"><span class="mrow" id="MathJax-Span-33"><span class="mi" id="MathJax-Span-34" style="font-family: MathJax_Main-bold;">v</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.807em; left: 0.598em;"><span class="mi" id="MathJax-Span-35" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-36"><span style="display: inline-block; position: relative; width: 1.313em; height: 0px;"><span style="position: absolute; clip: rect(3.098em 1000.84em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-37"><span class="mrow" id="MathJax-Span-38"><span class="mi" id="MathJax-Span-39" style="font-family: MathJax_Main-bold;">U</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.807em; left: 0.896em;"><span class="texatom" id="MathJax-Span-40"><span class="mrow" id="MathJax-Span-41"><span class="mi" id="MathJax-Span-42" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-43" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1009.47em 4.467em -999.997em); top: -2.557em; left: 50%; margin-left: -4.759em;"><span class="mspace" id="MathJax-Span-44" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span class="msubsup" id="MathJax-Span-45"><span style="display: inline-block; position: relative; width: 1.074em; height: 0px;"><span style="position: absolute; clip: rect(3.336em 1000.6em 4.348em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-46"><span class="mrow" id="MathJax-Span-47"><span class="mi" id="MathJax-Span-48" style="font-family: MathJax_Main-bold;">p</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.747em; left: 0.658em;"><span class="texatom" id="MathJax-Span-49"><span class="mrow" id="MathJax-Span-50"><span class="mi" id="MathJax-Span-51" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-52" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-53" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-54" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-55" style="font-family: MathJax_Main; padding-left: 0.301em;">=</span><span class="mi" id="MathJax-Span-56" style="font-family: MathJax_Main; padding-left: 0.301em;">exp</span><span class="mo" id="MathJax-Span-57"></span><span class="mo" id="MathJax-Span-58" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-59"><span style="display: inline-block; position: relative; width: 0.896em; height: 0px;"><span style="position: absolute; clip: rect(3.336em 1000.42em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-60"><span class="mrow" id="MathJax-Span-61"><span class="mi" id="MathJax-Span-62" style="font-family: MathJax_Main-bold;">s</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.807em; left: 0.479em;"><span class="texatom" id="MathJax-Span-63"><span class="mrow" id="MathJax-Span-64"><span class="mi" id="MathJax-Span-65" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-66" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-67" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-68" style="font-family: MathJax_Main;">)</span><span class="msubsup" id="MathJax-Span-69"><span style="display: inline-block; position: relative; width: 1.61em; height: 0px;"><span style="position: absolute; clip: rect(3.098em 1001.19em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-70"><span class="mrow" id="MathJax-Span-71"><span class="mi" id="MathJax-Span-72" style="font-family: MathJax_Main-bold;">W</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.807em; left: 1.193em;"><span class="texatom" id="MathJax-Span-73"><span class="mrow" id="MathJax-Span-74"><span class="mi" id="MathJax-Span-75" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-76" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(2.801em 1008.04em 4.408em -999.997em); top: -0.83em; left: 50%; margin-left: -4.045em;"><span class="mspace" id="MathJax-Span-77" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span class="texatom" id="MathJax-Span-78"><span class="mrow" id="MathJax-Span-79"><span class="texatom" id="MathJax-Span-80"><span class="mrow" id="MathJax-Span-81"><span class="munderover" id="MathJax-Span-82"><span style="display: inline-block; position: relative; width: 0.658em; height: 0px;"><span style="position: absolute; clip: rect(3.098em 1000.6em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="mi" id="MathJax-Span-83" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.455em 1000.48em 3.991em -999.997em); top: -4.64em; left: 0.063em;"><span class="mo" id="MathJax-Span-84" style="font-family: MathJax_Main-bold;">~</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span></span></span></span><span class="mo" id="MathJax-Span-85" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-86" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-87" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-88" style="font-family: MathJax_Main; padding-left: 0.301em;">=</span><span class="texatom" id="MathJax-Span-89" style="padding-left: 0.301em;"><span class="mrow" id="MathJax-Span-90"><span class="mi" id="MathJax-Span-91" style="font-family: MathJax_Main-bold;">h</span></span></span><span class="mo" id="MathJax-Span-92" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-93" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-94" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-95" style="font-family: MathJax_Main; padding-left: 0.241em;">∗</span><span class="msubsup" id="MathJax-Span-96" style="padding-left: 0.241em;"><span style="display: inline-block; position: relative; width: 1.074em; height: 0px;"><span style="position: absolute; clip: rect(3.336em 1000.6em 4.348em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-97"><span class="mrow" id="MathJax-Span-98"><span class="mi" id="MathJax-Span-99" style="font-family: MathJax_Main-bold;">p</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.747em; left: 0.658em;"><span class="texatom" id="MathJax-Span-100"><span class="mrow" id="MathJax-Span-101"><span class="mi" id="MathJax-Span-102" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-103" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-104" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-105" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 5.122em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -3.622em; border-left: 0px solid; width: 0px; height: 4.441em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">s</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">h</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">W</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo>+</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>a</mi></msub><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">U</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">p</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">s</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">W</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>p</mi></mrow></msub><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mover><mi mathvariant="bold">h</mi><mo mathvariant="bold" stretchy="false">~</mo></mover></mrow></mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">h</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>∗</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">p</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-1">{\bf s}_{a}(t) = \tanh({\bf h}(t) {\bf W}_{a} + {\bf v}_a {\bf U}_{a})\\
{\bf p}_{a}(t) = \exp({\bf s}_{a}(t) {\bf W}_{p})\\
{\bf \tilde{h}}(t) = {\bf h}(t) * {\bf p}_{a} (t)</script>

<p>The attention parameter is a function of the current hidden state and the attention vector mixed together. Each is first put through a matrix, summed and put through an activation function to get an attention state, which is then put through another transformation to get an attention parameter. The attention parameter then re-updates the hidden state. Supposedly, this is conceptually similar to TF-IDF weighting, where the model learns to weight particular states at particular times.</p>

<h2 id="building-an-attentional-lstm-example">Building an Attentional LSTM Example</h2>

<p>Now that we have all the components for an Attentional LSTM, let’s see the code for how we could implement this. The attentional component can be tacked onto the LSTM code that already exists.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>

<span class="k">class</span> <span class="nc">AttentionLSTM</span><span class="p">(</span><span class="n">LSTM</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">attention_vec</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_vec</span> <span class="o">=</span> <span class="n">attention_vec</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_vec</span><span class="p">,</span> <span class="s">'_keras_shape'</span><span class="p">)</span>
        <span class="n">attention_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_vec</span><span class="o">.</span><span class="n">_keras_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">U_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_init</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
                                   <span class="n">name</span><span class="o">=</span><span class="s">'{}_U_a'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_a</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">'{}_b_a'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">U_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_init</span><span class="p">((</span><span class="n">attention_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
                                   <span class="n">name</span><span class="o">=</span><span class="s">'{}_U_m'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_m</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">'{}_b_m'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">U_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_init</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
                                   <span class="n">name</span><span class="o">=</span><span class="s">'{}_U_s'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_s</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">'{}_b_s'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">U_a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_s</span><span class="p">,</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">b_a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_s</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initial_weights</span><span class="p">)</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_weights</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AttentionLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

        <span class="n">m</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_a</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_s</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_s</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="n">s</span>

        <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_constants</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">constants</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AttentionLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_constants</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">constants</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_vec</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_m</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_m</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">constants</span></code></pre></figure>

<p>Let’s look at what each function is doing individually. Note that this builds heavily upon the already-existing LSTM implementation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span></code></pre></figure>

<p>We will create a subclass (does python even do subclasses?) of the LSTM implementation that Keras already provides. The Keras <code class="highlighter-rouge">backend</code> is either Theano or Tensorflow, depending on the settings specified in <code class="highlighter-rouge">~/.keras/keras.json</code> (the default is Theano). This backend lets us use Theano-type functions such as <code class="highlighter-rouge">K.zeros</code>, which specifies a matrix of zeros, to initialize our model.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">attention_vec</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_vec</span> <span class="o">=</span> <span class="n">attention_vec</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentionLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code></pre></figure>

<p>We initialize the layer by passing it the out number of hidden layers <code class="highlighter-rouge">output_dim</code> and the layer to use as the attention vector <code class="highlighter-rouge">attention_vec</code>. The <code class="highlighter-rouge">__init__</code> function is identical to the <code class="highlighter-rouge">__init__</code> function for the <code class="highlighter-rouge">LSTM</code> layer except for the attention vector, so we just reuse it here.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span></code></pre></figure>

<p>I won’t reproduce everything here, but essentially this method initializes all of the weight matrices we need for the attentional component, after calling the <code class="highlighter-rouge">LSTM.build</code> method to initialize the LSTM weight matrices.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
    <span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AttentionLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_a</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_s</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_s</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="n">s</span>

    <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span></code></pre></figure>

<p>This method is used by the <code class="highlighter-rouge">RNN</code> superclass, and tells the function what to do on each timestep. It mirrors the equations given earlier, and adds the attentional component on top of the LSTM equations.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_constants</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">constants</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AttentionLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_constants</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">constants</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_vec</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_m</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_m</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">constants</span></code></pre></figure>

<p>This method is used by the LSTM superclass to define components outside of the step function, so that they don’t need to be recomputed every time step. In our case, the attentional vector doesn’t need to be recomputed every time step, so we define it as a constant (we then grab it in the <code class="highlighter-rouge">step</code> function using <code class="highlighter-rouge">attention = states[4]</code>).</p>

<h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>

<p>Convolutional networks are better explained elsewhere, and all of the functions required for making a good CNN language model are already supported in Keras. Basically, with language modeling, a common strategy is to apply a ton (on the order of 1000) convolutional filters to the embedding layer followed by a max-1 pooling function and call it a day. It actually works stupidly well for question answering (see <a href="http://arxiv.org/pdf/1508.01585v2.pdf">Feng et. al.</a> for benchmarks). This approach can be done fairly easily in Keras. One thing that may not be intuitive, however, is how to combine several filter lengths. This can be done as follows:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Convolution1D</span>

<span class="n">cnns</span> <span class="o">=</span> <span class="p">[</span><span class="n">Convolution1D</span><span class="p">(</span><span class="n">filter_length</span><span class="o">=</span><span class="n">filt</span><span class="p">,</span> <span class="n">nb_filter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">border_mode</span><span class="o">=</span><span class="s">'same'</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">filt</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]</span>
<span class="n">question</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">cnn</span><span class="p">(</span><span class="n">question</span><span class="p">)</span> <span class="k">for</span> <span class="n">cnn</span> <span class="ow">in</span> <span class="n">cnns</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">'concat'</span><span class="p">)</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">cnn</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span> <span class="k">for</span> <span class="n">cnn</span> <span class="ow">in</span> <span class="n">cnns</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">'concat'</span><span class="p">)</span></code></pre></figure>

<h1 id="similarity-metrics">Similarity Metrics</h1>

<p>The basic idea with question answering is to embed questions and answers as vectors, so that the question vector is close in vector space to the answer vector. For example, with the Attentional RNN, we take the question vector and use it as an input for generating the answer vector. A common approach is to then rank answer vectors according to their cosine similarities with the question vector. This doesn’t follow the conventional neural network architecture, and takes some manipulation to achieve in Keras. To use equations, what we would like to do is:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">best answer <span class="o">=</span> argmax<span class="o">(</span>cos<span class="o">(</span>question, answers<span class="o">))</span></code></pre></figure>

<p>Training is generally done by minimizing hinge loss. In this case, we want the cosine similarity for the correct answer to go up, and the cosine similarity for an incorrect answer to go down. The loss function can be formulated as:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">loss <span class="o">=</span> max<span class="o">(</span>0, constant margin - cos<span class="o">(</span>question, good answer<span class="o">)</span> + cos<span class="o">(</span>question, bad answer<span class="o">))</span></code></pre></figure>

<p>Note that for some implementations, having a loss of zero can be troublesome, so a small value like <code class="highlighter-rouge">1e-6</code> is preferable instead. The loss is zero when the difference between the cosine similarities of the good and bad answers is greater than the constant margin we defined. In practice, the margins generally range from 0.001 to 0.2. If we want to use something besides cosine similarity, we can reformulate this as</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">loss <span class="o">=</span> max<span class="o">(</span>0, constant margin - sim<span class="o">(</span>question, good answer<span class="o">)</span> + sim<span class="o">(</span>question, bad answer<span class="o">))</span></code></pre></figure>

<p>where <code class="highlighter-rouge">sim</code> is our similarity metric. Hinge loss works well for this application, as opposed to something like mean squared error, because we don’t want our question vectors to be orthogonal with the bad answer vectors, we just want the bad answer vectors to be a good distance away.</p>

<h2 id="cosine-similarity-example-rotational-matrix">Cosine Similarity Example: Rotational Matrix</h2>

<p>First, let’s look at how to do cosine similarity within the constraints of Keras. Fortunately, Keras has an implementation of cosine similarity, as a <code class="highlighter-rouge">mode</code> argument to the <code class="highlighter-rouge">merge</code> layer. This is done with:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">merge</span>
<span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">'cos'</span><span class="p">,</span> <span class="n">dot_axes</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code></pre></figure>

<p>If we pass it two inputs of dimensions <code class="highlighter-rouge">(a, b, c)</code>, it will calculate the cosine simliarity of the <code class="highlighter-rouge">c</code> dimension (specified using <code class="highlighter-rouge">dot_axes</code>) and give an output of dimensions <code class="highlighter-rouge">(a, b)</code>. However, because we might eventually want to implement other types of similarities besides cosine similarity, let’s look at how this can be done by passing a lambda function to <code class="highlighter-rouge">merge</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>
<span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="n">similarity</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></code></pre></figure>

<p>We define a function <code class="highlighter-rouge">similarity</code> which we will use to compute the similarity of the inputs passed to the <code class="highlighter-rouge">merge</code> layer. Note that when we do this, we also have to pass an <code class="highlighter-rouge">output_shape</code> which tells Keras what shape the output will be after we do this operation (hopefully in the future this shape will be inferred, but it is still an open issue in the Github group).</p>

<p>A cool example might be to see if we can learn a rotation matrix. A rotation matrix in Euclidean space is a matrix which rotates a vector by a certain angle around the origin. It is defined as a function of <code class="highlighter-rouge">theta</code>, the angle to rotate by:</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mtable rowspacing=&quot;4pt&quot; columnspacing=&quot;1em&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mi&gt;cos&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;sin&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mi&gt;sin&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mi&gt;cos&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-106" style="width: 11.134em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.598em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(1.729em 1010.36em 4.586em -999.997em); top: -3.39em; left: 0em;"><span class="mrow" id="MathJax-Span-107"><span class="mi" id="MathJax-Span-108" style="font-family: MathJax_Math-italic;">R</span><span class="mo" id="MathJax-Span-109" style="font-family: MathJax_Main; padding-left: 0.301em;">=</span><span class="mrow" id="MathJax-Span-110" style="padding-left: 0.301em;"><span class="mo" id="MathJax-Span-111" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">[</span></span><span class="mtable" id="MathJax-Span-112" style="padding-right: 0.182em; padding-left: 0.182em;"><span style="display: inline-block; position: relative; width: 7.027em; height: 0px;"><span style="position: absolute; clip: rect(2.384em 1002.5em 5.182em -999.997em); top: -3.985em; left: 0em;"><span style="display: inline-block; position: relative; width: 2.562em; height: 0px;"><span style="position: absolute; clip: rect(3.039em 1002.5em 4.408em -999.997em); top: -4.699em; left: 50%; margin-left: -1.307em;"><span class="mtd" id="MathJax-Span-113"><span class="mrow" id="MathJax-Span-114"><span class="mi" id="MathJax-Span-115" style="font-family: MathJax_Main;">cos</span><span class="mo" id="MathJax-Span-116"></span><span class="mo" id="MathJax-Span-117" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-118" style="font-family: MathJax_Math-italic;">θ</span><span class="mo" id="MathJax-Span-119" style="font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1002.38em 4.408em -999.997em); top: -3.211em; left: 50%; margin-left: -1.247em;"><span class="mtd" id="MathJax-Span-128"><span class="mrow" id="MathJax-Span-129"><span class="mi" id="MathJax-Span-130" style="font-family: MathJax_Main;">sin</span><span class="mo" id="MathJax-Span-131"></span><span class="mo" id="MathJax-Span-132" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-133" style="font-family: MathJax_Math-italic;">θ</span><span class="mo" id="MathJax-Span-134" style="font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(2.384em 1003.34em 5.182em -999.997em); top: -3.985em; left: 3.574em;"><span style="display: inline-block; position: relative; width: 3.455em; height: 0px;"><span style="position: absolute; clip: rect(3.039em 1003.34em 4.408em -999.997em); top: -4.699em; left: 50%; margin-left: -1.723em;"><span class="mtd" id="MathJax-Span-120"><span class="mrow" id="MathJax-Span-121"><span class="mo" id="MathJax-Span-122" style="font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-123" style="font-family: MathJax_Main; padding-left: 0.182em;">sin</span><span class="mo" id="MathJax-Span-124"></span><span class="mo" id="MathJax-Span-125" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-126" style="font-family: MathJax_Math-italic;">θ</span><span class="mo" id="MathJax-Span-127" style="font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1002.5em 4.408em -999.997em); top: -3.211em; left: 50%; margin-left: -1.307em;"><span class="mtd" id="MathJax-Span-135"><span class="mrow" id="MathJax-Span-136"><span class="mi" id="MathJax-Span-137" style="font-family: MathJax_Main;">cos</span><span class="mo" id="MathJax-Span-138"></span><span class="mo" id="MathJax-Span-139" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-140" style="font-family: MathJax_Math-italic;">θ</span><span class="mo" id="MathJax-Span-141" style="font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-142" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">]</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.396em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.753em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>R</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing="4pt" columnspacing="1em"><mtr><mtd><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mo>−</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd></mtr></mtable><mo>]</mo></mrow></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-2">% <![CDATA[
R = \begin{bmatrix}\cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta)\end{bmatrix} %]]></script>

<p>We can learn this matrix really simply with the right dataset and one dense layer, that is:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">'a'</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>

<span class="n">a_rotated</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="n">a_rotated</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">a_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">b_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">a_data</span><span class="p">],</span> <span class="p">[</span><span class="n">b_data</span><span class="p">],</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">get_value</span><span class="p">())</span></code></pre></figure>

<p>A <code class="highlighter-rouge">Dense</code> layer with <code class="highlighter-rouge">linear</code> activation is the exact same as a matrix multiplication. We give it two input dimensions and two output dimensions. After training this model, the printed weight matrix is:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[[</span>-0.00603954, -0.99370766]
 <span class="o">[</span> 0.99173903,  0.0078686 <span class="o">]]</span></code></pre></figure>

<p>which is close to the rotation matrix for an angle of 90 degrees. Let’s try this again, but with cosine similarity. This will require some manipulation. In the previous example, we had a clearly defined input, <code class="highlighter-rouge">a</code>, and output, <code class="highlighter-rouge">b</code>, and our model was designed to perform a transformation on <code class="highlighter-rouge">a</code> to predict <code class="highlighter-rouge">b</code>. In this example, we have two inputs, <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>, and we will perform a transformation on <code class="highlighter-rouge">a</code> to make it close to <code class="highlighter-rouge">b</code>. As an output, we get the similarity of the two vectors, so we need to train our model to make this similarity high by providing it a bunch of 1’s as the target values, since a similarity of 1 indicates perfect similarity.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">merge</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">'a'</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>

<span class="n">a_rotated</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cosine</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_keras_shape</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
    <span class="n">dot</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">a_rotated</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="n">cosine</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="n">cosine_sim</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">a_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">b_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">a_data</span><span class="p">,</span> <span class="n">b_data</span><span class="p">],</span> <span class="p">[</span><span class="n">targets</span><span class="p">],</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">get_value</span><span class="p">())</span></code></pre></figure>

<p>Running this, we end up with a weight matrix that looks like</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[[</span>-0.16537911 -1.26961863]
 <span class="o">[</span> 1.06261277  0.1144496 <span class="o">]]</span></code></pre></figure>

<p>This looks a bit like cosine similarity, but the scaling seems off. Cosine similarity is ambivalent about the magnitude of vectors, so the weight matrix ends up not being a rotation matrix so much as a rotation-and-skew matrix. It is interesting to think about why this network learned this particular matrix.</p>

<p>Below, a unit square (blue) is multiplied by the first matrix to get the orange square, and by the second matrix to get the yellow square.</p>

<p><img src="./Deep Language Modeling for Question Answering using Keras_files/matrix_transform.png" alt="Matrix transformation"></p>

<h2 id="other-similarity-metrics">Other Similarity Metrics</h2>

<p><a href="http://arxiv.org/pdf/1508.01585v2.pdf">Feng et. al.</a> provided a list of similarities along with their benchmarks for a CNN architecture. Some of these similarities, along with their implementations in Keras, are reproduced below. They rely on these helper functions:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="n">axis</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">_keras_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">dot</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axis</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="n">l2_norm</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span></code></pre></figure>

<p>If the function requires extra parameters, they are usually supplied as arguments in a dictionary.</p>

<h3 id="cosine">Cosine</h3>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-143" style="width: 3.812em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.634em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(0.36em 1003.63em 3.277em -999.997em); top: -2.08em; left: 0em;"><span class="mrow" id="MathJax-Span-144"><span class="mfrac" id="MathJax-Span-145"><span style="display: inline-block; position: relative; width: 3.396em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(2.979em 1001.67em 4.348em -999.997em); top: -4.699em; left: 50%; margin-left: -0.83em;"><span class="mrow" id="MathJax-Span-146"><span class="mi" id="MathJax-Span-147" style="font-family: MathJax_Math-italic;">x</span><span class="msubsup" id="MathJax-Span-148"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.396em 1000.48em 4.348em -999.997em); top: -3.985em; left: 0em;"><span class="mi" id="MathJax-Span-149" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.342em; left: 0.539em;"><span class="mi" id="MathJax-Span-150" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.063em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1003.16em 4.408em -999.997em); top: -3.271em; left: 50%; margin-left: -1.664em;"><span class="mrow" id="MathJax-Span-151"><span class="texatom" id="MathJax-Span-152"><span class="mrow" id="MathJax-Span-153"><span class="mo" id="MathJax-Span-154" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-155"><span class="mrow" id="MathJax-Span-156"><span class="mo" id="MathJax-Span-157" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-158" style="font-family: MathJax_Math-italic;">x</span><span class="texatom" id="MathJax-Span-159"><span class="mrow" id="MathJax-Span-160"><span class="mo" id="MathJax-Span-161" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-162"><span class="mrow" id="MathJax-Span-163"><span class="mo" id="MathJax-Span-164" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-165"><span class="mrow" id="MathJax-Span-166"><span class="mo" id="MathJax-Span-167" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-168"><span class="mrow" id="MathJax-Span-169"><span class="mo" id="MathJax-Span-170" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-171" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-172"><span class="mrow" id="MathJax-Span-173"><span class="mo" id="MathJax-Span-174" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-175"><span class="mrow" id="MathJax-Span-176"><span class="mo" id="MathJax-Span-177" style="font-family: MathJax_Main;">|</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1003.4em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.396em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.086em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.753em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mrow><mi>x</mi><msup><mi>y</mi><mi>T</mi></msup></mrow><mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></mfrac></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-3">\frac{x y^T}{||x|| ||y||}</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">cosine</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span></code></pre></figure>

<h3 id="polynomial">Polynomial</h3>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B3;&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-178" style="width: 5.42em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.122em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(1.134em 1005.12em 2.622em -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-179"><span class="mo" id="MathJax-Span-180" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-181" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-182" style="font-family: MathJax_Math-italic;">x</span><span class="msubsup" id="MathJax-Span-183"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.396em 1000.48em 4.348em -999.997em); top: -3.985em; left: 0em;"><span class="mi" id="MathJax-Span-184" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 0.539em;"><span class="mi" id="MathJax-Span-185" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.063em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-186" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="mi" id="MathJax-Span-187" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">c</span><span class="msubsup" id="MathJax-Span-188"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.039em 1000.3em 4.408em -999.997em); top: -3.985em; left: 0em;"><span class="mo" id="MathJax-Span-189" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 0.42em;"><span class="mi" id="MathJax-Span-190" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo stretchy="false">(</mo><mi>γ</mi><mi>x</mi><msup><mi>y</mi><mi>T</mi></msup><mo>+</mo><mi>c</mi><msup><mo stretchy="false">)</mo><mi>d</mi></msup></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-4">(\gamma x y^T + c)^d</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">polynomial</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">params</span><span class="p">[</span><span class="s">'c'</span><span class="p">])</span> <span class="o">**</span> <span class="n">params</span><span class="p">[</span><span class="s">'d'</span><span class="p">]</span></code></pre></figure>

<p>Values for <code class="highlighter-rouge">gamma</code> used in the paper were <code class="highlighter-rouge">[0.5,  1.0, 1.5]</code>. The value for <code class="highlighter-rouge">c</code> was usually <code class="highlighter-rouge">1</code>. Values for <code class="highlighter-rouge">d</code> were <code class="highlighter-rouge">[2, 3]</code>.</p>

<h3 id="sigmoid">Sigmoid</h3>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;tanh&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B3;&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-191" style="width: 7.027em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.67em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(1.134em 1006.55em 2.622em -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-192"><span class="mi" id="MathJax-Span-193" style="font-family: MathJax_Main;">tanh</span><span class="mo" id="MathJax-Span-194"></span><span class="mo" id="MathJax-Span-195" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-196" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-197" style="font-family: MathJax_Math-italic;">x</span><span class="msubsup" id="MathJax-Span-198"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.396em 1000.48em 4.348em -999.997em); top: -3.985em; left: 0em;"><span class="mi" id="MathJax-Span-199" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.402em; left: 0.539em;"><span class="mi" id="MathJax-Span-200" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.063em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-201" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="mi" id="MathJax-Span-202" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">c</span><span class="mo" id="MathJax-Span-203" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>γ</mi><mi>x</mi><msup><mi>y</mi><mi>T</mi></msup><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-5">\tanh(\gamma x y^T + c)</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">params</span><span class="p">[</span><span class="s">'c'</span><span class="p">])</span></code></pre></figure>

<p>Values for <code class="highlighter-rouge">gamma</code> used in the paper were <code class="highlighter-rouge">[0.5, 1.0, 1.5]</code>, and <code class="highlighter-rouge">c</code> was <code class="highlighter-rouge">1</code>.</p>

<h3 id="rbf">RBF</h3>

<p>RBF stands for radial basis function.</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B3;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-204" style="width: 7.86em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.443em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(1.074em 1007.32em 2.622em -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-205"><span class="mi" id="MathJax-Span-206" style="font-family: MathJax_Main;">exp</span><span class="mo" id="MathJax-Span-207"></span><span class="mo" id="MathJax-Span-208" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-209" style="font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-210" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-211"><span class="mrow" id="MathJax-Span-212"><span class="mo" id="MathJax-Span-213" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-214"><span class="mrow" id="MathJax-Span-215"><span class="mo" id="MathJax-Span-216" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-217" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-218" style="font-family: MathJax_Main; padding-left: 0.241em;">−</span><span class="mi" id="MathJax-Span-219" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-220"><span class="mrow" id="MathJax-Span-221"><span class="mo" id="MathJax-Span-222" style="font-family: MathJax_Main;">|</span></span></span><span class="msubsup" id="MathJax-Span-223"><span style="display: inline-block; position: relative; width: 0.717em; height: 0px;"><span style="position: absolute; clip: rect(3.039em 1000.18em 4.408em -999.997em); top: -3.985em; left: 0em;"><span class="texatom" id="MathJax-Span-224"><span class="mrow" id="MathJax-Span-225"><span class="mo" id="MathJax-Span-226" style="font-family: MathJax_Main;">|</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.461em; left: 0.301em;"><span class="mn" id="MathJax-Span-227" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-228" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>γ</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mo>−</mo><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-6">\exp(-\gamma ||x - y||^2)</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">rbf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="n">l2_norm</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span></code></pre></figure>

<p>Values for <code class="highlighter-rouge">gamma</code> used in the paper were <code class="highlighter-rouge">[0.5, 1.0, 1.5]</code>.</p>

<h3 id="euclidean">Euclidean</h3>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-229" style="width: 5.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.539em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(0.539em 1005.54em 3.277em -999.997em); top: -2.08em; left: 0em;"><span class="mrow" id="MathJax-Span-230"><span class="mfrac" id="MathJax-Span-231"><span style="display: inline-block; position: relative; width: 5.301em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(3.158em 1000.42em 4.17em -999.997em); top: -4.64em; left: 50%; margin-left: -0.235em;"><span class="mn" id="MathJax-Span-232" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1005.06em 4.408em -999.997em); top: -3.271em; left: 50%; margin-left: -2.616em;"><span class="mrow" id="MathJax-Span-233"><span class="mn" id="MathJax-Span-234" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-235" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="texatom" id="MathJax-Span-236" style="padding-left: 0.241em;"><span class="mrow" id="MathJax-Span-237"><span class="mo" id="MathJax-Span-238" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-239"><span class="mrow" id="MathJax-Span-240"><span class="mo" id="MathJax-Span-241" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-242" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-243" style="font-family: MathJax_Main; padding-left: 0.241em;">−</span><span class="mi" id="MathJax-Span-244" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-245"><span class="mrow" id="MathJax-Span-246"><span class="mo" id="MathJax-Span-247" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-248"><span class="mrow" id="MathJax-Span-249"><span class="mo" id="MathJax-Span-250" style="font-family: MathJax_Main;">|</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1005.3em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.301em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.086em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.566em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mo>−</mo><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></mfrac></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-7">\frac{1}{1 + ||x - y||}</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">euclidean</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">l2_norm</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span></code></pre></figure>

<h3 id="exponential">Exponential</h3>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B3;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-251" style="width: 7.384em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.027em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(1.253em 1006.91em 2.622em -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-252"><span class="mi" id="MathJax-Span-253" style="font-family: MathJax_Main;">exp</span><span class="mo" id="MathJax-Span-254"></span><span class="mo" id="MathJax-Span-255" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-256" style="font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-257" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-258"><span class="mrow" id="MathJax-Span-259"><span class="mo" id="MathJax-Span-260" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-261"><span class="mrow" id="MathJax-Span-262"><span class="mo" id="MathJax-Span-263" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-264" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-265" style="font-family: MathJax_Main; padding-left: 0.241em;">−</span><span class="mi" id="MathJax-Span-266" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-267"><span class="mrow" id="MathJax-Span-268"><span class="mo" id="MathJax-Span-269" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-270"><span class="mrow" id="MathJax-Span-271"><span class="mo" id="MathJax-Span-272" style="font-family: MathJax_Main;">|</span></span></span><span class="mo" id="MathJax-Span-273" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>γ</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mo>−</mo><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-8">\exp(-\gamma ||x - y||)</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">exponential</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="n">l2_norm</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span></code></pre></figure>

<h3 id="gesd">GESD</h3>

<p>This was a custom metric developed by the authors which stands for Geometric mean of Euclidean and Sigmoid Dot product. It performed well for their benchmarks.</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B3;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-274" style="width: 17.265em; display: inline-block;"><span style="display: inline-block; position: relative; width: 16.432em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(0.658em 1016.43em 3.396em -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-275"><span class="mfrac" id="MathJax-Span-276"><span style="display: inline-block; position: relative; width: 5.301em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(3.158em 1000.42em 4.17em -999.997em); top: -4.64em; left: 50%; margin-left: -0.235em;"><span class="mn" id="MathJax-Span-277" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1005.06em 4.408em -999.997em); top: -3.271em; left: 50%; margin-left: -2.616em;"><span class="mrow" id="MathJax-Span-278"><span class="mn" id="MathJax-Span-279" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-280" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="texatom" id="MathJax-Span-281" style="padding-left: 0.241em;"><span class="mrow" id="MathJax-Span-282"><span class="mo" id="MathJax-Span-283" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-284"><span class="mrow" id="MathJax-Span-285"><span class="mo" id="MathJax-Span-286" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-287" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-288" style="font-family: MathJax_Main; padding-left: 0.241em;">−</span><span class="mi" id="MathJax-Span-289" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-290"><span class="mrow" id="MathJax-Span-291"><span class="mo" id="MathJax-Span-292" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-293"><span class="mrow" id="MathJax-Span-294"><span class="mo" id="MathJax-Span-295" style="font-family: MathJax_Main;">|</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1005.3em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.301em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span><span class="mo" id="MathJax-Span-296" style="font-family: MathJax_Main; padding-left: 0.241em;">∗</span><span class="mfrac" id="MathJax-Span-297" style="padding-left: 0.241em;"><span style="display: inline-block; position: relative; width: 9.646em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(3.158em 1000.42em 4.17em -999.997em); top: -4.64em; left: 50%; margin-left: -0.235em;"><span class="mn" id="MathJax-Span-298" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1009.47em 4.408em -999.997em); top: -3.211em; left: 50%; margin-left: -4.759em;"><span class="mrow" id="MathJax-Span-299"><span class="mn" id="MathJax-Span-300" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-301" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="mi" id="MathJax-Span-302" style="font-family: MathJax_Main; padding-left: 0.241em;">exp</span><span class="mo" id="MathJax-Span-303"></span><span class="mo" id="MathJax-Span-304" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-305" style="font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-306" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-307" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-308" style="font-family: MathJax_Math-italic;">x</span><span class="msubsup" id="MathJax-Span-309"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.396em 1000.48em 4.348em -999.997em); top: -3.985em; left: 0em;"><span class="mi" id="MathJax-Span-310" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.283em; left: 0.539em;"><span class="mi" id="MathJax-Span-311" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.063em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-312" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="mi" id="MathJax-Span-313" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">c</span><span class="mo" id="MathJax-Span-314" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-315" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1009.65em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 9.646em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.566em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mo>−</mo><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></mfrac><mo>∗</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>γ</mi><mo stretchy="false">(</mo><mi>x</mi><msup><mi>y</mi><mi>T</mi></msup><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-9">\frac{1}{1 + ||x - y||} * \frac{1}{1 + \exp(-\gamma (x y^T + c))}</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gesd</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">euclidean</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">l2_norm</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">sigmoid</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">params</span><span class="p">[</span><span class="s">'c'</span><span class="p">])))</span>
    <span class="k">return</span> <span class="n">euclidean</span> <span class="o">*</span> <span class="n">sigmoid</span></code></pre></figure>

<p>Values for <code class="highlighter-rouge">gamma</code> used were <code class="highlighter-rouge">[0.5, 1.0, 1.5]</code> and <code class="highlighter-rouge">c</code> was <code class="highlighter-rouge">1</code>.</p>

<h3 id="aesd">AESD</h3>

<p>This was a custom metric developed by the authors which stands for Arithmetic mean of Euclidean and Sigmoid Dot product. It performed well for their benchmarks.</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;0.5&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;0.5&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B3;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-316" style="width: 17.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 16.67em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(0.658em 1016.67em 3.396em -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-317"><span class="mfrac" id="MathJax-Span-318"><span style="display: inline-block; position: relative; width: 5.301em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(3.158em 1001.25em 4.17em -999.997em); top: -4.64em; left: 50%; margin-left: -0.652em;"><span class="mn" id="MathJax-Span-319" style="font-family: MathJax_Main;">0.5</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1005.06em 4.408em -999.997em); top: -3.271em; left: 50%; margin-left: -2.616em;"><span class="mrow" id="MathJax-Span-320"><span class="mn" id="MathJax-Span-321" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-322" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="texatom" id="MathJax-Span-323" style="padding-left: 0.241em;"><span class="mrow" id="MathJax-Span-324"><span class="mo" id="MathJax-Span-325" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-326"><span class="mrow" id="MathJax-Span-327"><span class="mo" id="MathJax-Span-328" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-329" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-330" style="font-family: MathJax_Main; padding-left: 0.241em;">−</span><span class="mi" id="MathJax-Span-331" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-332"><span class="mrow" id="MathJax-Span-333"><span class="mo" id="MathJax-Span-334" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-335"><span class="mrow" id="MathJax-Span-336"><span class="mo" id="MathJax-Span-337" style="font-family: MathJax_Main;">|</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1005.3em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.301em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span><span class="mo" id="MathJax-Span-338" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="mfrac" id="MathJax-Span-339" style="padding-left: 0.241em;"><span style="display: inline-block; position: relative; width: 9.646em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(3.158em 1001.25em 4.17em -999.997em); top: -4.64em; left: 50%; margin-left: -0.652em;"><span class="mn" id="MathJax-Span-340" style="font-family: MathJax_Main;">0.5</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1009.47em 4.408em -999.997em); top: -3.211em; left: 50%; margin-left: -4.759em;"><span class="mrow" id="MathJax-Span-341"><span class="mn" id="MathJax-Span-342" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-343" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="mi" id="MathJax-Span-344" style="font-family: MathJax_Main; padding-left: 0.241em;">exp</span><span class="mo" id="MathJax-Span-345"></span><span class="mo" id="MathJax-Span-346" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-347" style="font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-348" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-349" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-350" style="font-family: MathJax_Math-italic;">x</span><span class="msubsup" id="MathJax-Span-351"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.396em 1000.48em 4.348em -999.997em); top: -3.985em; left: 0em;"><span class="mi" id="MathJax-Span-352" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -4.283em; left: 0.539em;"><span class="mi" id="MathJax-Span-353" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.063em;"></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="mo" id="MathJax-Span-354" style="font-family: MathJax_Main; padding-left: 0.241em;">+</span><span class="mi" id="MathJax-Span-355" style="font-family: MathJax_Math-italic; padding-left: 0.241em;">c</span><span class="mo" id="MathJax-Span-356" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-357" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1009.65em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 9.646em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.566em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mn>0.5</mn><mrow><mn>1</mn><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mo>−</mo><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></mfrac><mo>+</mo><mfrac><mn>0.5</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>γ</mi><mo stretchy="false">(</mo><mi>x</mi><msup><mi>y</mi><mi>T</mi></msup><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-10">\frac{0.5}{1 + ||x - y||} + \frac{0.5}{1 + \exp(-\gamma (x y^T + c))}</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gesd</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">euclidean</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">l2_norm</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">sigmoid</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">params</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">params</span><span class="p">[</span><span class="s">'c'</span><span class="p">])))</span>
    <span class="k">return</span> <span class="n">euclidean</span> <span class="o">+</span> <span class="n">sigmoid</span></code></pre></figure>

<p>Values for <code class="highlighter-rouge">gamma</code> used were <code class="highlighter-rouge">[0.5, 1.0, 1.5]</code> and <code class="highlighter-rouge">c</code> was <code class="highlighter-rouge">1</code>.</p>

<h1 id="insuranceqa-model-example">InsuranceQA Model Example</h1>

<p>A surprisingly good model for the <a href="http://arxiv.org/pdf/1508.01585v2.pdf">InsuranceQA dataset</a> is as follows:</p>

<p><img src="./Deep Language Modeling for Question Answering using Keras_files/model_diagram.jpeg" alt="Model diagram"></p>

<p>This model achieved relatively good marks for Top-1 Accuracy (how often did the model rank a ground truth the highest out of 500 results) and Mean Reciprocal Rank (MRR), which is defined as</p>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-358" style="width: 10.777em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.241em; height: 0px; font-size: 105%;"><span style="position: absolute; clip: rect(0.182em 1010.24em 3.574em -999.997em); top: -2.199em; left: 0em;"><span class="mrow" id="MathJax-Span-359"><span class="mi" id="MathJax-Span-360" style="font-family: MathJax_Math-italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.063em;"></span></span><span class="mi" id="MathJax-Span-361" style="font-family: MathJax_Math-italic;">R</span><span class="mi" id="MathJax-Span-362" style="font-family: MathJax_Math-italic;">R</span><span class="mo" id="MathJax-Span-363" style="font-family: MathJax_Main; padding-left: 0.301em;">=</span><span class="mfrac" id="MathJax-Span-364" style="padding-left: 0.301em;"><span style="display: inline-block; position: relative; width: 1.491em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(3.158em 1000.42em 4.17em -999.997em); top: -4.64em; left: 50%; margin-left: -0.235em;"><span class="mn" id="MathJax-Span-365" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.039em 1001.25em 4.408em -999.997em); top: -3.271em; left: 50%; margin-left: -0.652em;"><span class="mrow" id="MathJax-Span-366"><span class="texatom" id="MathJax-Span-367"><span class="mrow" id="MathJax-Span-368"><span class="mo" id="MathJax-Span-369" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-370" style="font-family: MathJax_Math-italic;">Q</span><span class="texatom" id="MathJax-Span-371"><span class="mrow" id="MathJax-Span-372"><span class="mo" id="MathJax-Span-373" style="font-family: MathJax_Main;">|</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1001.49em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.491em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span><span class="munderover" id="MathJax-Span-374" style="padding-left: 0.182em;"><span style="display: inline-block; position: relative; width: 1.432em; height: 0px;"><span style="position: absolute; clip: rect(2.86em 1001.37em 4.646em -999.997em); top: -3.985em; left: 0em;"><span class="mo" id="MathJax-Span-375" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.336em 1001.07em 4.289em -999.997em); top: -2.914em; left: 0.122em;"><span class="texatom" id="MathJax-Span-376"><span class="mrow" id="MathJax-Span-377"><span class="mi" id="MathJax-Span-378" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-379" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-380" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.158em 1000.9em 4.348em -999.997em); top: -5.235em; left: 0.241em;"><span class="texatom" id="MathJax-Span-381"><span class="mrow" id="MathJax-Span-382"><span class="texatom" id="MathJax-Span-383"><span class="mrow" id="MathJax-Span-384"><span class="mo" id="MathJax-Span-385" style="font-size: 70.7%; font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-386" style="font-size: 70.7%; font-family: MathJax_Math-italic;">Q</span><span class="texatom" id="MathJax-Span-387"><span class="mrow" id="MathJax-Span-388"><span class="mo" id="MathJax-Span-389" style="font-size: 70.7%; font-family: MathJax_Main;">|</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span><span class="texatom" id="MathJax-Span-390" style="padding-left: 0.182em;"><span class="mrow" id="MathJax-Span-391"><span class="mfrac" id="MathJax-Span-392"><span style="display: inline-block; position: relative; width: 2.562em; height: 0px; margin-right: 0.122em; margin-left: 0.122em;"><span style="position: absolute; clip: rect(3.158em 1000.42em 4.17em -999.997em); top: -4.64em; left: 50%; margin-left: -0.235em;"><span class="mn" id="MathJax-Span-393" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(3.098em 1002.44em 4.348em -999.997em); top: -3.271em; left: 50%; margin-left: -1.188em;"><span class="mrow" id="MathJax-Span-394"><span class="mi" id="MathJax-Span-395" style="font-family: MathJax_Math-italic;">r</span><span class="mi" id="MathJax-Span-396" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-397" style="font-family: MathJax_Math-italic;">n</span><span class="msubsup" id="MathJax-Span-398"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.098em 1000.48em 4.17em -999.997em); top: -3.985em; left: 0em;"><span class="mi" id="MathJax-Span-399" style="font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; top: -3.807em; left: 0.539em;"><span class="mi" id="MathJax-Span-400" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.991em;"></span></span><span style="position: absolute; clip: rect(0.836em 1002.56em 1.253em -999.997em); top: -1.307em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.562em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.074em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.309em; border-left: 0px solid; width: 0px; height: 3.316em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>M</mi><mi>R</mi><mi>R</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></mfrac><munderover><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mfrac><mn>1</mn><mrow><mi>r</mi><mi>a</mi><mi>n</mi><msub><mi>k</mi><mi>i</mi></msub></mrow></mfrac></mrow></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-11">MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|}{\frac{1}{rank_i}}</script>

<p>The results after learning the training set are summaraized in the following table.</p>

<table>
  <thead>
    <tr>
      <th>Test set</th>
      <th>Top-1 Accuracy</th>
      <th>Mean Reciprocal Rank</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Test 1</td>
      <td>0.4933</td>
      <td>0.6189</td>
    </tr>
    <tr>
      <td>Test 2</td>
      <td>0.4606</td>
      <td>0.5968</td>
    </tr>
    <tr>
      <td>Dev</td>
      <td>0.4700</td>
      <td>0.6088</td>
    </tr>
  </tbody>
</table>

<p>For comparison, the best model from <a href="http://arxiv.org/pdf/1508.01585v2.pdf">Feng et. al.</a> achieved an accuracy of 0.653 on Test 1, and the model in <a href="http://arxiv.org/pdf/1511.04108.pdf">Tan et. al.</a> achieved an accuracy of 0.681 on Test 1. This model isn’t exceptional, but it works pretty well for how simple it is.  It outperforms the baseline bag of words model, and performs on par with the Metzler-Bendersky IR model introduced in “Learning concept importance using a weighted dependence model” (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.2597&amp;rep=rep1&amp;type=pdf">Bendersky and Metzler, 2010</a>). Here’s how we build it in Keras:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build</span><span class="p">():</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">sentence_length</span><span class="p">,))</span>

    <span class="c"># embedding</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">n_words</span><span class="p">,</span> <span class="n">n_embed_dims</span><span class="p">)</span>
    <span class="n">input_embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="c"># dropout</span>
    <span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">input_dropout</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">)</span>

    <span class="c"># maxpooling</span>
    <span class="n">maxpool</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                     <span class="n">output_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="n">input_pool</span> <span class="o">=</span> <span class="n">maxpool</span><span class="p">(</span><span class="n">input_dropout</span><span class="p">)</span>

    <span class="c"># activation</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s">'tanh'</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">input_pool</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="nb">input</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">build</span><span class="p">()</span>
<span class="n">question</span><span class="p">,</span> <span class="n">answer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">q_len</span><span class="p">,)),</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">a_len</span><span class="p">,))</span>

<span class="n">question_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="n">answer_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>

<span class="n">similarity</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">question_output</span><span class="p">,</span> <span class="n">answer_output</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">'cos'</span><span class="p">,</span> <span class="n">dot_axes</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Merge</span><span class="p">([</span><span class="n">question_output</span><span class="p">,</span> <span class="n">answer_output</span><span class="p">],</span> <span class="p">[</span><span class="n">similarity</span><span class="p">])</span></code></pre></figure>

<p>The code is kind of awkward without the context, so I would recommend checking out the repository to see how it works. The repository contains the necessary code for building a question answering model using Keras and evaluating it on the Insurance QA dataset.</p>

<h1 id="discussion-links">Discussion Links</h1>

<ul>
  <li><a href="https://www.reddit.com/r/MachineLearning/comments/4h3moa/deep_language_modeling_for_question_answering/">/r/MachineLearning</a></li>
  <li><a href="https://news.ycombinator.com/item?id=11623287">Hacker News</a></li>
</ul>


    
    <sub style="text-align: right;">
    	<div>Send corrections to <a href="mailto:bkbolte18@gmail.com">bkbolte18@gmail.com</a> ◦ <a href="http://benjaminbolte.com/blog/2016/keras-language-modeling.html#top">Back to top</a></div>
    </sub>
  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li id="site-title">Byens Bsolete's Bloog</li>
          <li><a href="mailto:bkbolte18@gmail.com">bkbolte18@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/codekansas">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                </svg>
              </span>

              <span class="username">codekansas</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/code__kansas">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
                </svg>
              </span>

              <span class="username">code__kansas</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Another machine learning blog.
</p>
      </div>
    </div>

  </div>
 
 <script type="text/javascript">
 var tagline = document.getElementById("site-title");
 var letters = "Ben Bolte's Blog".split("");
 var punctuation = "eeeeeeeeeeeetttttttttaaaaaaaaooooooooiiiiiiinnnnnnnsssssshhhhhhrrrrrrddddllllcccuuummwwffggyyppbvk".split(""); // Approximates the frequency of English letter usage
 var txt = "";
 
 for (var i = 0; i < letters.length; i++) {
  txt += letters[i];
 	if (letters[i] != ' ' && letters[i] != '\'' && letters[i] != 's') {
 	  if (Math.random() > 0.7) {
   	  txt += punctuation[Math.floor(Math.random()*punctuation.length)];
   	}
 	}
 }
 
 tagline.innerHTML = txt;
 </script>

</footer>


  


<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_Size2, sans-serif;"></div></div></body></html>